{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3bf6bc",
   "metadata": {},
   "source": [
    "# Lab 2 fashion MNIST\n",
    "\n",
    "Este notebook es una variacion del proyecto 1 en el cual aplicaremos los mismos modelos que aplicamos pero ahora para un dataset distinto: fashion MNIST\n",
    "\n",
    "El cual consiste en 70,000 imagenes de prendas de vestir con su etiqueta del tipo de prenda que representan\n",
    "\n",
    "![Fashion MNIST](./Fashion_MNIST.png)\n",
    "\n",
    "Seguiremos la misma estructura que en en proyecto 1\n",
    "\n",
    "1. Read Data (Leer los datos)\n",
    "2. Data Preprocessing (Preprocesamiento de los datos)\n",
    "3. Model Creation (Creacion del modelo)\n",
    "4. Adjust Model with Historic Data (Ajustar el modelo con infromacion historica)\n",
    "5. Prediction from new Data (Predecir a partir de nueva informacion)\n",
    "6. Visualization of Results (Visualizar los resultados)\n",
    "\n",
    "\n",
    "Para correr este notebook es necesario tener: python3, anaconda y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42243690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830359c2",
   "metadata": {},
   "source": [
    "## Leer los datos\n",
    "\n",
    "Este es un paso sencillo, ya que las librerias que usamos contienen el data set en ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666ac20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 7s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8962a",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "en este caso solo preparamos la informacion para poder utilizarla despues en SKLEARN que la necesita en un arreglo unidimencional de 784 (28x28) entradas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15a2a429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5af64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reshape, for SKLEARN usage\n",
    "Skdata_X = X_train.reshape(60000,784)\n",
    "Skdata_X_test = X_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d7e82",
   "metadata": {},
   "source": [
    "## Creacion del modelo Y ajuste del modelo\n",
    "\n",
    "Para este dataset tenemos planeado utilizar 5 modelos\n",
    "\n",
    "#### Modelos a usar\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Random Forest\n",
    "- Neural Network (MLP)\n",
    "- CNN (Deep Learning)\n",
    "\n",
    "En cada seccion creamos el modelo y lo ajustamos (entrenamos) con la informacion de entrenamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aaaf47",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd88955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavosotres/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Creation\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "mnist_logistic = LogisticRegression()\n",
    "mnist_logistic.fit(Skdata_X,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc3d08",
   "metadata": {},
   "source": [
    "### SVM - Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d285323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gustavosotres/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "mnist_svm = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5))\n",
    "# Model Training\n",
    "mnist_svm.fit(Skdata_X,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ecbb0a",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe718b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "mnist_randForest = RandomForestClassifier(n_estimators=10)\n",
    "mnist_randForest.fit(Skdata_X,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c546654",
   "metadata": {},
   "source": [
    "### Neural Network - MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cdf7687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-18 23:53:37.056899: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 3.0103 - accuracy: 0.6910 - val_loss: 0.8660 - val_accuracy: 0.7283\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.6858 - accuracy: 0.7671 - val_loss: 0.8063 - val_accuracy: 0.7597\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.6082 - accuracy: 0.7900 - val_loss: 0.6476 - val_accuracy: 0.7556\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5463 - accuracy: 0.8126 - val_loss: 0.5963 - val_accuracy: 0.8037\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5310 - accuracy: 0.8143 - val_loss: 0.6743 - val_accuracy: 0.7994\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5267 - accuracy: 0.8173 - val_loss: 0.5447 - val_accuracy: 0.8117\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4984 - accuracy: 0.8267 - val_loss: 0.5167 - val_accuracy: 0.8172\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4853 - accuracy: 0.8330 - val_loss: 0.5478 - val_accuracy: 0.8119\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4802 - accuracy: 0.8381 - val_loss: 0.6285 - val_accuracy: 0.7923\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4754 - accuracy: 0.8382 - val_loss: 0.5282 - val_accuracy: 0.8278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb52a927190>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN (MLP)\n",
    "from tensorflow import keras\n",
    "model_nn = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "model_nn.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model_nn.fit(X_train, y_train, epochs=10,\n",
    "         validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cee418",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network - CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26945169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5539 - accuracy: 0.8215 - val_loss: 0.4099 - val_accuracy: 0.8534\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3383 - accuracy: 0.8773 - val_loss: 0.3527 - val_accuracy: 0.8691\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3023 - accuracy: 0.8902 - val_loss: 0.3401 - val_accuracy: 0.8776\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2782 - accuracy: 0.8970 - val_loss: 0.3135 - val_accuracy: 0.8896\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2579 - accuracy: 0.9045 - val_loss: 0.3142 - val_accuracy: 0.8915\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2396 - accuracy: 0.9120 - val_loss: 0.3118 - val_accuracy: 0.8934\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2266 - accuracy: 0.9155 - val_loss: 0.3099 - val_accuracy: 0.8883\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2184 - accuracy: 0.9191 - val_loss: 0.3172 - val_accuracy: 0.8891\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2114 - accuracy: 0.9218 - val_loss: 0.2982 - val_accuracy: 0.8951\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1988 - accuracy: 0.9258 - val_loss: 0.3468 - val_accuracy: 0.8921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb50198e550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN \n",
    "from tensorflow.keras import layers, models\n",
    "train_images = X_train.reshape(60000,28,28,1)\n",
    "test_images = X_test.reshape(10000,28,28,1)\n",
    "model_cnn = models.Sequential()\n",
    "model_cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "model_cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(layers.Flatten())\n",
    "model_cnn.add(layers.Dense(64, activation='relu'))\n",
    "model_cnn.add(layers.Dense(10))\n",
    "model_cnn.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "# Model Fitting\n",
    "model_cnn.fit(train_images, y_train, epochs=10, \n",
    "                    validation_data=(test_images, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba77e8",
   "metadata": {},
   "source": [
    "## Predecir con informacion nueva\n",
    "\n",
    "Es aqui donde la separacion de nuestra informacion en: entrenamiento y test muestra su importancia ya que si entrenaramos nuestro modelo con toda la informacion que tenemos el modelo siempre daria resultados positivos con ella ya que sabe que informacion recibira de entrada y como queremos provar como el modelo haria con informacion que nunca ha visto, entonces dividimos y entrenamos con una parte, para despues probar el modelo con con la otra parte, con informacion que nunca habia visto y asi tener una metrica mas correcta de que tan efectivo es.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d665572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 715us/step\n",
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Data Prediction\n",
    "# Logistic Regression\n",
    "fy_predict_logistic = mnist_logistic.predict(Skdata_X_test)\n",
    "# Support Vector Machine\n",
    "fy_predict_svm = mnist_svm.predict(Skdata_X_test)\n",
    "# Random Forest\n",
    "fy_predict_randForest = mnist_randForest.predict(Skdata_X_test)\n",
    "# MLP\n",
    "predict_nn = model_nn.predict(X_test) # Array que contiene los arrays con la probabilidad de \n",
    "                                        #que los datos de entrada pertenezcan a un label\n",
    "fy_predict_mlp = [] # Lista vacía, aquí se almacenarán los labels con la probabilidad más alta\n",
    "for i in range(len(predict_nn)): #Para cada array en predict_nn\n",
    "    fy_predict_mlp.append(np.argmax(predict_nn[i])) # Encuentra la posición de la prob más alta\n",
    "                                                    # y almacénala en la lista\n",
    "        \n",
    "#CNN \n",
    "predict_cnn = model_cnn.predict(X_test)\n",
    "fy_predict_cnn = []\n",
    "for i in range(len(predict_cnn)):\n",
    "    fy_predict_cnn.append(np.argmax(predict_cnn[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7800bd5",
   "metadata": {},
   "source": [
    "## Visualizacion de resultados\n",
    "\n",
    "Ahora que entrenamos nuestros modelos y obtuvimos resultados, es importante saber leer estos resultados. Esto nos permitira hacer un analizis mas presiso y entender si nuestro modelo es bueno o malo y asi ajustar algunos parametros que podemos controlar de los modelos.\n",
    "\n",
    "Utilizaremos cross validation para obtener 4 metricas\n",
    "\n",
    "- Un valor de exactitud\n",
    "- Un valor de presision\n",
    "- Un valor de **\\_** ( Recall xD)\n",
    "- El valor F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e5611fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83778f",
   "metadata": {},
   "source": [
    "Ahora vemos estos resultados para poder hacer los ajustes pertinentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b68d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [accuracy_score(y_test, fy_predict_logistic),\n",
    "            accuracy_score(y_test, fy_predict_svm),\n",
    "            accuracy_score(y_test, fy_predict_randForest),\n",
    "           accuracy_score(y_test, fy_predict_mlp),\n",
    "           accuracy_score(y_test, fy_predict_cnn)]\n",
    "precision = [precision_score(y_test, fy_predict_logistic, average='macro'),\n",
    "             precision_score(y_test, fy_predict_svm, average='macro'),\n",
    "             precision_score(y_test, fy_predict_randForest, average='macro'),\n",
    "            precision_score(y_test, fy_predict_mlp, average='macro'),\n",
    "            precision_score(y_test, fy_predict_cnn, average='macro')]\n",
    "recall = [recall_score(y_test, fy_predict_logistic, average='micro'),\n",
    "         recall_score(y_test, fy_predict_svm, average='micro'),\n",
    "         recall_score(y_test, fy_predict_randForest, average='micro'),\n",
    "         recall_score(y_test, fy_predict_mlp, average='micro'),\n",
    "         recall_score(y_test, fy_predict_cnn, average='micro')]\n",
    "f1 = [f1_score(y_test, fy_predict_logistic, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_svm, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_randForest, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_mlp, average = 'weighted'),\n",
    "     f1_score(y_test, fy_predict_cnn, average = 'weighted')]\n",
    "perf_metrics = pd.DataFrame(data = [accuracy,precision,recall,f1], columns = [\"Logistic\",\n",
    "                                                                              \"SVM\", \"RandForest\",\"MLP\",\"CNN\"],\n",
    "                           index = [\"Accuracy\",\"Precision\",\"Recall\",\"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc5e514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic</th>\n",
       "      <th>SVM</th>\n",
       "      <th>RandForest</th>\n",
       "      <th>MLP</th>\n",
       "      <th>CNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.841200</td>\n",
       "      <td>0.822900</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.827800</td>\n",
       "      <td>0.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.839748</td>\n",
       "      <td>0.822886</td>\n",
       "      <td>0.854374</td>\n",
       "      <td>0.831422</td>\n",
       "      <td>0.893324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.841200</td>\n",
       "      <td>0.822900</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.827800</td>\n",
       "      <td>0.892100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.839852</td>\n",
       "      <td>0.821377</td>\n",
       "      <td>0.853466</td>\n",
       "      <td>0.828334</td>\n",
       "      <td>0.892477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Logistic       SVM  RandForest       MLP       CNN\n",
       "Accuracy   0.841200  0.822900    0.855300  0.827800  0.892100\n",
       "Precision  0.839748  0.822886    0.854374  0.831422  0.893324\n",
       "Recall     0.841200  0.822900    0.855300  0.827800  0.892100\n",
       "F1         0.839852  0.821377    0.853466  0.828334  0.892477"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3eb825599874f73d53a28eb1517ebf1ba28ce5d8e92d7a88138417e905ce08d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
